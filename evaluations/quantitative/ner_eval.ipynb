{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63822fa2-f7f6-4178-ae3e-1e2618c02145",
   "metadata": {},
   "source": [
    "### NER Eval\n",
    "\n",
    "This is the notebook used to develop the code in ner_semeval.py.\n",
    "\n",
    "Uses the library NER-Evaluation developed by davidsbatista, available at https://github.com/davidsbatista/NER-Evaluation/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104772e0-8f80-4818-b06d-f432f3038529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn==1.5.0 in /afs/crc.nd.edu/user/k/kmealey2/.local/lib/python3.11/site-packages (1.5.0)\n",
      "Requirement already satisfied: nltk in /afs/crc.nd.edu/user/k/kmealey2/.local/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: sklearn_crfsuite in /afs/crc.nd.edu/user/k/kmealey2/.local/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /afs/crc.nd.edu/user/k/kmealey2/.local/lib/python3.11/site-packages (from scikit-learn==1.5.0) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /afs/crc.nd.edu/user/k/kmealey2/.local/lib/python3.11/site-packages (from scikit-learn==1.5.0) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn==1.5.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /afs/crc.nd.edu/user/k/kmealey2/.local/lib/python3.11/site-packages (from scikit-learn==1.5.0) (3.5.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.7 in /afs/crc.nd.edu/user/k/kmealey2/.local/lib/python3.11/site-packages (from sklearn_crfsuite) (0.9.10)\n",
      "Requirement already satisfied: tabulate>=0.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from sklearn_crfsuite) (0.8.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.5.0 nltk sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa0eff4-fc5e-4b00-8463-c022ab0477c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 19:57:30 numexpr.utils INFO: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-08-01 19:57:30 numexpr.utils INFO: NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./NER-Evaluation\")\n",
    "\n",
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "from ner_evaluation.ner_eval import compute_metrics\n",
    "from ner_evaluation.ner_eval import compute_precision_recall_wrapper\n",
    "from ner_evaluation.ner_eval import find_overlap\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88818303-49d6-4d19-8e33-e5563fb8baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Entity = namedtuple(\"Entity\", \"e_type start_offset end_offset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1a532-d253-4fd4-91ea-6dfe9c41ca01",
   "metadata": {},
   "source": [
    "### Implement our own version of collect_named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd180a8b-db8e-41c2-959f-3fd51bfd9177",
   "metadata": {},
   "source": [
    "**Use code from coref_reformat.ipynb from crosslingual_coref**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2549ef-33ca-46c9-a9ad-6abd9c2632e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get FAA data in format {c5_id:{0: word0, 1: word1, ..., n: wordn}} using word tokenization from faa.conll\n",
    "\n",
    "faa = {}\n",
    "\n",
    "with open('../../OMIn_dataset/data/FAA_data/faa.conll') as f:\n",
    "    text = f.read()\n",
    "\n",
    "docs = text.split('#begin document ')\n",
    "\n",
    "for doc in docs:\n",
    "    if doc[:5] == '(faa/':\n",
    "        word_count = 0\n",
    "        c5_id = doc.split('_')[1][:15]\n",
    "        faa[c5_id] = {}\n",
    "        lines = doc.split('\\n')\n",
    "        for line in lines[1:]:\n",
    "            if 'faa' in line:\n",
    "                faa[c5_id][word_count] = line.split()[3].upper()\n",
    "                word_count = word_count + 1\n",
    "# Fix known err\n",
    "faa['19980620030289I'] = {0: 'MR.', 1: 'KADERA', 2: 'THEN', 3: 'ATTEMPTED', 4: 'TO', 5: 'LAND', 6: 'IN', 7: 'A', 8: 'FIELD', 9: 'BUT', 10: 'WAS', 11: 'FORCED', 12: 'TO', 13: 'LAND', 14: 'ON', 15: 'HIGHWAY', 16: '93', 17: '.', 18: 'THREE', 19: 'MILES', 20: 'EAST', 21: 'OF', 22: 'SUNMER', 23: ',', 24: 'IOWA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "396f935f-be04-412d-aced-d9aea0c35f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['MR.', 'KADERA', 'THEN', 'ATTEMPTED', 'TO', 'LAND', 'IN', 'A', 'FIELD', 'BUT', 'WAS', 'FORCED', 'TO', 'LAND', 'ON', 'HIGHWAY', '93', '.', 'THREE', 'MILES', 'EAST', 'OF', 'SUNMER', ',', 'IOWA'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faa['19980620030289I'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef6226b0-2aa6-4f0d-b6de-459e6e8e06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(mentions, words):\n",
    "    ''' Input:\n",
    "    - mentions:['MENTION1','MENTION2',...]\n",
    "    - words: ['This','is','a','sentence','.','This','is','another','sentence','.'] (dict values)\n",
    "        Output: [[startidx_mention1, end_idxmention1], [startidx_mention2, end_idxmention2], ...]\n",
    "    '''\n",
    "\n",
    "    mention_spans = []\n",
    "\n",
    "    repeat_mentions = {}\n",
    "\n",
    "    if \"'S\" in words.values():\n",
    "        idx = list(words.values()).index(\"'S\")\n",
    "        words[idx-1] = words[idx-1] + \"'S\"\n",
    "        del words[idx]\n",
    "    \n",
    "    for imention, mention in enumerate(mentions):\n",
    "\n",
    "        mention = mention.replace('(', ' ( ').replace(')',' ) ').replace('  ',' ')\n",
    "        mention = mention.replace(',',' , ').replace('  ',' ')\n",
    "\n",
    "        mention_span = [-1, -1] # if conditions below aren't met, [-1,-1 is returned]\n",
    "\n",
    "        tokens = list(words.values())\n",
    "        idxs = list(words.keys())\n",
    "\n",
    "        # Check if mention has been seen before. If has, start_idx already stored in repeat_mentions\n",
    "        # Matched sequentially \n",
    "        if mention in repeat_mentions:\n",
    "            if len(repeat_mentions[mention]) > 0:\n",
    "                start_idx = repeat_mentions[mention].pop(0) # get start_idx and pop off list\n",
    "                end_idx = start_idx + len(mention.split()) - 1\n",
    "                mention_span = [idxs[start_idx],idxs[end_idx]+1]\n",
    "\n",
    "        # Normal case, where it has not been seen before, and we search for the start of the phrase in check_words\n",
    "        else:\n",
    "            start_indices = [i for i in range(len(tokens)) if tokens[i:i+len(mention.split())] == mention.split()]\n",
    "\n",
    "            # If start_indices contains multiple idxs, get start_idx from front of list (first occurance) and save rest to repeat_mentions\n",
    "            # If start_indices contains just one idx, that is the start_idx\n",
    "            if len(start_indices) > 0:\n",
    "                \n",
    "                if len(start_indices) > 1:\n",
    "                    repeat_mentions[mention] = start_indices[1:]\n",
    "                \n",
    "                start_idx = start_indices[0]\n",
    "                end_idx = start_idx + len(mention.split()) - 1\n",
    "                mention_span = [idxs[start_idx],idxs[end_idx]+1]\n",
    "\n",
    "        mention_spans.append(mention_span)\n",
    "\n",
    "    return mention_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0af1a9-49a2-42dc-bd01-b7ed5ab5a7db",
   "metadata": {},
   "source": [
    "**Now, very simple to make our own version of collect_named_entities()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664db2fc-e2ae-4029-929d-eee5f07d0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_named_entities(entities, labels, tokens):\n",
    "    \"\"\"\n",
    "    Creates a list of Entity named-tuples, storing the entity type and the start and end\n",
    "    offsets of the entity.\n",
    "\n",
    "    Parameters:\n",
    "    - entities: [\"ENT1\",\"ENT2\"...] All entities for a doc\n",
    "    - labels: [\"LABEL1\",\"LABEL2\"...] All corresponding labels for a doc\n",
    "    - tokens: dict_values(['TOW', 'PLANE', 'BECAME', ...]) Tokenized doc. Result of faa[doc_id].values()\n",
    "\n",
    "    Returns: a list of Entity named-tuples\n",
    "    \"\"\"\n",
    "\n",
    "    ent_spans = get_spans(entities, tokens)\n",
    "\n",
    "    named_entities = []\n",
    "    for ient, ent_span in enumerate(ent_spans):\n",
    "        named_entities.append(Entity(labels[ient], ent_span[0], ent_span[1]))\n",
    "\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d4e8b446-92d3-4786-9219-d970017354cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Entity(e_type='ORG', start_offset=0, end_offset=1),\n",
       " Entity(e_type='ORG', start_offset=4, end_offset=6),\n",
       " Entity(e_type='ORG', start_offset=8, end_offset=9),\n",
       " Entity(e_type='ORG', start_offset=13, end_offset=14),\n",
       " Entity(e_type='ORG', start_offset=-1, end_offset=-1),\n",
       " Entity(e_type='ORG', start_offset=18, end_offset=19)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_named_entities(gold_df[gold_df['id']=='19850315007389A']['entities'].to_list(),gold_df[gold_df['id']=='19850315007389A']['labels'].to_list(),faa['19850315007389A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d607eb-75f6-4a38-afbc-057599e3a3e2",
   "metadata": {},
   "source": [
    "### Get Predicted and Gold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab72d1ac-3abb-487a-b055-e27db6a65cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>c119_input</th>\n",
       "      <th>entities</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19750315005389A</td>\n",
       "      <td>TAILWHEEL COCKED RIGHT PRIOR TO TKOF.         ...</td>\n",
       "      <td>COCKED</td>\n",
       "      <td>PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19750419011349A</td>\n",
       "      <td>TOW PLANE BECAME AIRBORNE THEN SETTLED.STUDENT...</td>\n",
       "      <td>TOW PLANE BECAME AIRBORNE THEN SETTLED.STUDENT...</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19751029037799A</td>\n",
       "      <td>2ND ILS APCH,ACFT'S G/S INOP.LOM TUNED TO WRON...</td>\n",
       "      <td>2ND</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19751029037799A</td>\n",
       "      <td>2ND ILS APCH,ACFT'S G/S INOP.LOM TUNED TO WRON...</td>\n",
       "      <td>ACFT</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19751029037799A</td>\n",
       "      <td>2ND ILS APCH,ACFT'S G/S INOP.LOM TUNED TO WRON...</td>\n",
       "      <td>FREQ</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                         c119_input  \\\n",
       "0  19750315005389A  TAILWHEEL COCKED RIGHT PRIOR TO TKOF.         ...   \n",
       "1  19750419011349A  TOW PLANE BECAME AIRBORNE THEN SETTLED.STUDENT...   \n",
       "2  19751029037799A  2ND ILS APCH,ACFT'S G/S INOP.LOM TUNED TO WRON...   \n",
       "3  19751029037799A  2ND ILS APCH,ACFT'S G/S INOP.LOM TUNED TO WRON...   \n",
       "4  19751029037799A  2ND ILS APCH,ACFT'S G/S INOP.LOM TUNED TO WRON...   \n",
       "\n",
       "                                            entities   labels  \n",
       "0                                             COCKED  PRODUCT  \n",
       "1  TOW PLANE BECAME AIRBORNE THEN SETTLED.STUDENT...      ORG  \n",
       "2                                                2ND     DATE  \n",
       "3                                               ACFT      ORG  \n",
       "4                                               FREQ      PER  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbrevs = {'FACILITY':'FAC','ORGANIZATION':'ORG','PERSON':'PER','LOCATION':'LOC','VEH':'VEHICLE'}\n",
    "result_df = pd.read_csv('../../tool_results/spacy_entityrecognizer/spacy_ner_lg.csv')\n",
    "result_df['labels'] = result_df['labels'].apply(lambda x: abbrevs[x] if x in abbrevs else x)\n",
    "result_df.rename(columns={'c5_unique_id':'id', 'c5_id':'id'},inplace=True)\n",
    "result_df['entities'] = result_df['entities'].apply(str.upper)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5188d7e2-8c81-48ac-a7a3-7a79be703316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sample</th>\n",
       "      <th>entities</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19990213001379A</td>\n",
       "      <td>ACFT WAS TAXIING FOR TAKE OFF WHEN IT LOST CON...</td>\n",
       "      <td>ACFT</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19800217031649I</td>\n",
       "      <td>AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...</td>\n",
       "      <td>TAKEOFF</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19800217031649I</td>\n",
       "      <td>AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...</td>\n",
       "      <td>ENGINE</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19800217031649I</td>\n",
       "      <td>AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...</td>\n",
       "      <td>WING</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19800217031649I</td>\n",
       "      <td>AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...</td>\n",
       "      <td>FUEL TANK</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                             sample  \\\n",
       "0  19990213001379A  ACFT WAS TAXIING FOR TAKE OFF WHEN IT LOST CON...   \n",
       "1  19800217031649I  AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...   \n",
       "2  19800217031649I  AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...   \n",
       "3  19800217031649I  AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...   \n",
       "4  19800217031649I  AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...   \n",
       "\n",
       "    entities labels  \n",
       "0       ACFT    ORG  \n",
       "1    TAKEOFF    ORG  \n",
       "2     ENGINE    ORG  \n",
       "3       WING    ORG  \n",
       "4  FUEL TANK    ORG  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df = pd.read_csv('../../OMIn_dataset/gold_standard/processed/ner.csv')\n",
    "if 'labels' not in gold_df.columns:\n",
    "    gold_df['labels'] = ['ORG']*len(gold_df) # Add dummy labels for aviation mentions-only gs\n",
    "gold_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6dad753b-04db-4f22-863c-b124cfd2e9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1226"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_col = 'id'\n",
    "len(result_df[id_col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "578e29ea-8715-46b5-b632-d958863b13b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold_df['id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023a22e-4b0e-4ffe-af01-aeee59bab80f",
   "metadata": {},
   "source": [
    "**Create all_true_ents and all_pred_ents lists**\n",
    "\n",
    "In the example in example-full-named-entity-evaluation.ipynb, they use test_sents_labels and y_pred, which are lists of the tokens to input to collect_named_entities to get true and pred, respectively. However, our collect_named_entities() takes more input, so it's easier to do that process ahead of time, and have lists of true's and pred's ready to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ade105e4-959e-4e7c-9372-ebe09ba2cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_ents(true_ents):\n",
    "    final_ents = {ent:0 for ent in true_ents}\n",
    "    for ent_a in true_ents:\n",
    "        rest = [ent for ent in true_ents if ent != ent_a]\n",
    "        for ent_b in rest:\n",
    "            overlap = find_overlap(range(ent_a[1],ent_a[2]),range(ent_b[1],ent_b[2]))\n",
    "            #print(f\"Comparing {ent_a} with {ent_b}, overlap = {overlap}\")\n",
    "            if len(overlap) > 0 and len(range(ent_a[1],ent_a[2])) > len(range(ent_b[1],ent_b[2])):\n",
    "                final_ents[ent_a] += 1\n",
    "    return [ent[0] for ent in sorted(final_ents.items(), key=lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "816e428e-7227-4518-aca8-8ed29d1d25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_ents = []\n",
    "all_pred_ents = []\n",
    "\n",
    "for doc_id in gold_df['id'].unique():\n",
    "    true_rows = gold_df.dropna()[gold_df.dropna()['id']==doc_id]\n",
    "    pred_rows = result_df.dropna()[result_df.dropna()[id_col]==doc_id]\n",
    "\n",
    "    true_ents = collect_named_entities(true_rows['entities'].to_list(),true_rows['labels'].to_list(),faa[doc_id])\n",
    "    pred_ents = collect_named_entities(pred_rows['entities'].to_list(),pred_rows['labels'].to_list(),faa[doc_id])\n",
    "    \n",
    "    all_true_ents.append(sort_ents(true_ents))\n",
    "    all_pred_ents.append(pred_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0788059f-b8a0-47ba-9582-3b4702de2242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 19991230042089A\n",
      "[Entity(e_type='ORG', start_offset=-1, end_offset=-1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for mentions where no span could be found (would print out below if present):\n",
    "\n",
    "for idoc, doc_id in enumerate(gold_df['id'].unique()):\n",
    "    for ent in all_true_ents[idoc]:\n",
    "        if ent[1] == -1:\n",
    "            print(f\"True: {doc_id}\")\n",
    "            print(all_true_ents[idoc])\n",
    "            print()\n",
    "    for ent in all_pred_ents[idoc]:\n",
    "        if ent[1] == -1:\n",
    "            print(f\"Pred: {doc_id}\")\n",
    "            print(all_pred_ents[idoc])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a10989-debc-4445-a852-8250d9e84bc5",
   "metadata": {},
   "source": [
    "### Apply Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b47a5476-5de2-424d-844f-5222d227d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_tags = ['PER', 'ORG', 'MISC', 'LOC']\n",
    "ace_nlkt_tags = ['PER','ORG','LOC','FAC','GPE'] # RESTRICTED SET\n",
    "ace_tags = ['PER','ORG','LOC','FAC','GPE','VEHICLE'] # RESTRICTED SET\n",
    "on_tags = ['PER','ORG','LOC','FAC','GPE','PRODUCT','NORP','QUANTITY','EVENT','WORK_OF_ART','CARDINAL','DATE','PERCENT','TIME','ORDINAL','MONEY','LAW','LANGUAGE']\n",
    "scierc_tags = ['OtherScientificTerm', 'Method', 'Task', 'Material', 'Generic', 'Metric','ORG'] # adding \"ORG\" which is a placeholder so that it will include all results in calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0515caa-5a88-495c-8a88-23fdbd5d0a50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags = scierc_tags\n",
    "\n",
    "metrics_results = {'correct': 0, 'incorrect': 0, 'partial': 0,\n",
    "                   'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0}\n",
    "\n",
    "# overall results\n",
    "results = {'strict': deepcopy(metrics_results),\n",
    "           'ent_type': deepcopy(metrics_results),\n",
    "           'partial':deepcopy(metrics_results),\n",
    "           'exact':deepcopy(metrics_results)\n",
    "          }\n",
    "\n",
    "\n",
    "# results aggregated by entity type\n",
    "evaluation_agg_entities_type = {e: deepcopy(results) for e in tags}\n",
    "\n",
    "for true_ents, pred_ents in zip(all_true_ents, all_pred_ents):\n",
    "    \n",
    "    # compute results for one message\n",
    "    tmp_results, tmp_agg_results = compute_metrics(\n",
    "        true_ents, pred_ents,  tags\n",
    "    )\n",
    "    \n",
    "    #print(tmp_results)\n",
    "\n",
    "    # aggregate overall results\n",
    "    for eval_schema in results.keys():\n",
    "        for metric in metrics_results.keys():\n",
    "            results[eval_schema][metric] += tmp_results[eval_schema][metric]\n",
    "            \n",
    "    # Calculate global precision and recall\n",
    "        \n",
    "    results = compute_precision_recall_wrapper(results)\n",
    "\n",
    "\n",
    "    # aggregate results by entity type\n",
    " \n",
    "    for e_type in tags:\n",
    "\n",
    "        for eval_schema in tmp_agg_results[e_type]:\n",
    "\n",
    "            for metric in tmp_agg_results[e_type][eval_schema]:\n",
    "                \n",
    "                evaluation_agg_entities_type[e_type][eval_schema][metric] += tmp_agg_results[e_type][eval_schema][metric]\n",
    "                \n",
    "        # Calculate precision recall at the individual entity level\n",
    "                \n",
    "        evaluation_agg_entities_type[e_type] = compute_precision_recall_wrapper(evaluation_agg_entities_type[e_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8775604c-61d1-4809-ab59-20c1efc37c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_labeled(tool_name, results):\n",
    "\n",
    "    scores = {'exact':0.0,'strict':0.0,'partial':0.0,'ent_type':0.0}\n",
    "    for score in scores:\n",
    "        prec = results[score]['precision']\n",
    "        rec = results[score]['recall']\n",
    "        scores[score] = f\"{2*prec*rec/(prec+rec):.4}\" \n",
    "\n",
    "    print('|                                         | Strict  | Exact  | Partial  | Type    |')\n",
    "    print('|-----------------------------------------|---------|--------|----------|---------|')\n",
    "    print(f\"| {tool_name:40}| {scores['strict']:8}| {scores['exact']:7}| {scores['partial']:9}| {scores['ent_type']:8}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e534d2f1-c52b-44bd-b465-f7396b9fb5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_unlabeled(tool_name, results):\n",
    "    scores = {'exact':0.0,'partial':0.0}\n",
    "    for score in scores:\n",
    "        prec = results[score]['precision']\n",
    "        rec = results[score]['recall']\n",
    "        scores[score] = {'prec':f\"{prec:.4}\", 'rec':f\"{rec:.4}\", 'f1':f\"{(2*prec*rec/(prec+rec)):.4}\"}\n",
    "\n",
    "    print('|                                         | Precision (Weak) | Recall (Weak) | F1 (Weak)     | Precision (Strong) | Recall (Strong) | F1 (Strong) |')\n",
    "    print('|-----------------------------------------|------------------|---------------|---------------|--------------------|-----------------|-------------|')\n",
    "    print(f\"| {tool_name:40}| {scores['partial']['prec']:17}| {scores['partial']['rec']:14}| {scores['partial']['f1']:14}| {scores['exact']['prec']:19}| {scores['exact']['rec']:16}| {scores['exact']['f1']:12}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0c46baf-b2b5-47c7-830e-91565e965062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                         | Precision (Weak) | Recall (Weak) | F1 (Weak)     | Precision (Strong) | Recall (Strong) | F1 (Strong) |\n",
      "|-----------------------------------------|------------------|---------------|---------------|--------------------|-----------------|-------------|\n",
      "| nltk (uppercased)                       | 0.6944           | 0.07367       | 0.1332        | 0.463              | 0.04912         | 0.08881     |\n"
     ]
    }
   ],
   "source": [
    "print_results_unlabeled('nltk (uppercased)',results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4fe83b6-d37e-495a-8608-d0d609bbb690",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m print_results_labeled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnltk (uppercased)\u001b[39m\u001b[38;5;124m'\u001b[39m,results)\n",
      "Cell \u001b[0;32mIn[50], line 7\u001b[0m, in \u001b[0;36mprint_results_labeled\u001b[0;34m(tool_name, results)\u001b[0m\n\u001b[1;32m      5\u001b[0m     prec \u001b[38;5;241m=\u001b[39m results[score][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m     rec \u001b[38;5;241m=\u001b[39m results[score][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m     scores[score] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mprec\u001b[38;5;241m*\u001b[39mrec\u001b[38;5;241m/\u001b[39m(prec\u001b[38;5;241m+\u001b[39mrec)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|                                         | Strict  | Exact  | Partial  | Type    |\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|-----------------------------------------|---------|--------|----------|---------|\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "print_results_labeled('nltk (uppercased)',results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "728edf0d-de34-4ffc-bd8f-2e36dd4a3e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ent_type': {'correct': 11,\n",
       "  'incorrect': 67,\n",
       "  'partial': 0,\n",
       "  'missed': 55,\n",
       "  'spurious': 380,\n",
       "  'possible': 133,\n",
       "  'actual': 458,\n",
       "  'precision': 0.024017467248908297,\n",
       "  'recall': 0.08270676691729323},\n",
       " 'partial': {'correct': 36,\n",
       "  'incorrect': 0,\n",
       "  'partial': 42,\n",
       "  'missed': 55,\n",
       "  'spurious': 380,\n",
       "  'possible': 133,\n",
       "  'actual': 458,\n",
       "  'precision': 0.12445414847161572,\n",
       "  'recall': 0.42857142857142855},\n",
       " 'strict': {'correct': 3,\n",
       "  'incorrect': 75,\n",
       "  'partial': 0,\n",
       "  'missed': 55,\n",
       "  'spurious': 380,\n",
       "  'possible': 133,\n",
       "  'actual': 458,\n",
       "  'precision': 0.006550218340611353,\n",
       "  'recall': 0.022556390977443608},\n",
       " 'exact': {'correct': 36,\n",
       "  'incorrect': 42,\n",
       "  'partial': 0,\n",
       "  'missed': 55,\n",
       "  'spurious': 380,\n",
       "  'possible': 133,\n",
       "  'actual': 458,\n",
       "  'precision': 0.07860262008733625,\n",
       "  'recall': 0.2706766917293233}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f4e186f-838d-43a5-9d0f-05b49a72cf95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def orig_calculate_precision_recall_f1(gs, df_tool, id_col, ent_col, matching=\"STRONG\"):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall based on entities comparison between gs (ground truth) and df_tool (answers).\n",
    "    \n",
    "    Parameters:\n",
    "    - gs: DataFrame with columns ['id', 'sample', 'entities'] representing the ground truth.\n",
    "    - df_tool: DataFrame with columns ['id', 'sample', 'entities', 'labels'] representing the tool's answers.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing precision and recall.\n",
    "    \"\"\"\n",
    "    TP = 0  # True Positives\n",
    "    FP = 0  # False Positives\n",
    "    FN = 0  # False Negatives\n",
    "    \n",
    "    # Check for True Positives and False Negatives by iterating over gs\n",
    "    for index, gs_row in gs.dropna().iterrows():\n",
    "        gs_id, gs_entity = gs_row['id'], gs_row['entities']\n",
    "        tool_entities = [entity.upper() for entity in df_tool.loc[df_tool[id_col] == gs_id, ent_col].tolist()] # get all the entities the tool generated for the gs_id entry\n",
    "        \n",
    "        # In strong matching, we only count a tool-generated entity as correct if it exactly matches the gold standard entity\n",
    "        if matching==\"STRONG\":\n",
    "            if gs_entity in tool_entities:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        # In weak matching, we count a tool-generated entity as correct if it is a subspan of the gold standard entity, or if the gold standard entity is a subspan of it\n",
    "        elif matching==\"WEAK\":\n",
    "            if any(gs_entity in tool_entity or tool_entity in gs_entity for tool_entity in tool_entities):\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            print(\"Error: Matching must = STRONG or WEAK\")\n",
    "            return None\n",
    "    \n",
    "    # Check for False Positives by iterating over df_tool\n",
    "    for index, tool_row in df_tool[df_tool[id_col].isin(gs['id'].unique())].iterrows():\n",
    "        tool_id, tool_entity = tool_row[id_col], tool_row[ent_col]\n",
    "        gs_entities = gs.dropna().loc[gs.dropna()['id'] == tool_id, 'entities'].tolist()\n",
    "\n",
    "        #  strong matching\n",
    "        if matching==\"STRONG\":\n",
    "            if tool_entity not in gs_entities:\n",
    "                FP += 1\n",
    "        else:\n",
    "            if not any(tool_entity in gs_entity or gs_entity in tool_entity for gs_entity in gs_entities):\n",
    "                FP += 1\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    #print(f\"TP={TP}, FP={FP}, FN={FN}, prec={precision}, rec={recall}\")\n",
    "    \n",
    "    # Calculating the F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return TP, FP, FN, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6d11a9fd-e2b5-4205-aeb6-0bf636dd3b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 374, 48, 0.16143497757847533, 0.6, 0.254416961130742)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_calculate_precision_recall_f1(pd.read_csv('../../OMIn_dataset/gold_standard/processed/ner_ace_nltk.csv'), pd.read_csv('../../OMIn_dataset/data/results/nltk/nltk_ner_uppercased.csv'), id_col, 'entities', matching=\"WEAK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ea34e91-b9d2-4a12-b2c3-35e823458b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlap(true_range, pred_range):\n",
    "    \"\"\"Find the overlap between two ranges\n",
    "\n",
    "    Find the overlap between two ranges. Return the overlapping values if\n",
    "    present, else return an empty set().\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    >>> find_overlap((1, 2), (2, 3))\n",
    "    2\n",
    "    >>> find_overlap((1, 2), (3, 4))\n",
    "    set()\n",
    "    \"\"\"\n",
    "\n",
    "    true_set = set(true_range)\n",
    "    pred_set = set(pred_range)\n",
    "\n",
    "    overlaps = true_set.intersection(pred_set)\n",
    "\n",
    "    return overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d91e7565-802f-458e-8655-c7fc2d9da6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_f1(all_true_ents, all_pred_ents, matching=\"STRONG\"):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall based on entities comparison between gs (ground truth) and df_tool (answers).\n",
    "    \n",
    "    Parameters:\n",
    "    - gs: DataFrame with columns ['id', 'sample', 'entities'] representing the ground truth.\n",
    "    - df_tool: DataFrame with columns ['id', 'sample', 'entities', 'labels'] representing the tool's answers.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing precision and recall.\n",
    "    \"\"\"\n",
    "    COR = 0  # correct\n",
    "    PAR = 0\n",
    "    FP = 0  # False Positives\n",
    "    FN = 0  # False Negatives\n",
    "\n",
    "    # Go doc by doc:\n",
    "    for true_ents, pred_ents in zip(all_true_ents, all_pred_ents):\n",
    "\n",
    "        unmatched_preds = [(ent[1],ent[2]) for ent in pred_ents] # copy\n",
    "\n",
    "        # Check CORs, PARs, and FNs by iterating over true ents\n",
    "        for true_ent in true_ents:\n",
    "            true_bounds = (true_ent[1],true_ent[2])\n",
    "            found_match = False\n",
    "            if true_bounds in unmatched_preds:\n",
    "                COR +=1\n",
    "                found_match = True\n",
    "                unmatched_preds.remove(true_bounds)\n",
    "            else:\n",
    "                for pred_ent in unmatched_preds:\n",
    "                    pred_bounds = (pred_ent[0],pred_ent[1])\n",
    "                    overlaps = find_overlap(true_bounds, pred_bounds)\n",
    "                    if len(overlaps) > 0:\n",
    "                        PAR += 1\n",
    "                        found_match = True\n",
    "                        unmatched_preds.remove(pred_ent)\n",
    "                        break\n",
    "            if not found_match:\n",
    "                FN += 1\n",
    "\n",
    "        FP += len(unmatched_preds)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    if matching==\"STRONG\":\n",
    "        TP = COR\n",
    "        FP += PAR\n",
    "        FN += PAR\n",
    "    else:\n",
    "        TP = PAR + COR\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    #print(f\"TP={TP}, FP={FP}, FN={FN}, prec={precision}, rec={recall}\")\n",
    "    \n",
    "    # Calculating the F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return COR, PAR, FP, FN, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b4651865-df9a-4217-acde-ca32e030e1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 39, 379, 41, 0.17248908296943233, 0.6583333333333333, 0.27335640138408307)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_precision_recall_f1(all_true_ents, all_pred_ents,matching=\"WEAK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "dfe653ca-1fa3-40ea-a018-3fc5f06e974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "davids = {'exact':{'correct':[],'incorrect':[],'partial':[],'missed':[],'spurious':[],'precision':[],'recall':[],'actual':[],'possible':[]},'partial':{'correct':[],'incorrect':[],'partial':[],'missed':[],'spurious':[],'precision':[],'recall':[],'actual':[],'possible':[]}}\n",
    "my_new = {'strong':{'COR':[],'PAR':[],'FP':[],'FN':[],'precision':[],'recall':[]},'weak':{'COR':[],'PAR':[],'FP':[],'FN':[],'precision':[],'recall':[]}}\n",
    "my_old = {'strong':{'TP':[],'FP':[],'FN':[],'precision':[],'recall':[]},'weak':{'TP':[],'FP':[],'FN':[],'precision':[],'recall':[]}}\n",
    "\n",
    "for idoc, doc_id in enumerate(gold_df['id'].unique()):\n",
    "\n",
    "    # David's way:\n",
    "    true_ents = all_true_ents[idoc]\n",
    "    pred_ents = all_pred_ents[idoc]\n",
    "    tmp_results, tmp_agg_results = compute_metrics(\n",
    "        true_ents, pred_ents,  tags\n",
    "    )\n",
    "    for type in ['exact','partial']:\n",
    "        for key in tmp_results[type].keys():\n",
    "            davids[type][key].append(tmp_results[type][key])\n",
    "\n",
    "    # My way:\n",
    "    gs = gold_df[gold_df['id']==doc_id].reset_index(drop=True)\n",
    "    df_tool = result_df[result_df[id_col]==doc_id].reset_index(drop=True)\n",
    "    for type in ['strong','weak']:\n",
    "        TP, FP, FN, prec, rec, f1 = orig_calculate_precision_recall_f1(gs, df_tool, id_col,'entities',matching=type.upper())\n",
    "        my_old[type]['TP'].append(TP)\n",
    "        my_old[type]['FP'].append(FP)\n",
    "        my_old[type]['FN'].append(FN)\n",
    "        my_old[type]['precision'].append(prec)\n",
    "        my_old[type]['recall'].append(rec)\n",
    "        \n",
    "        COR, PAR, FP, FN, prec, rec, f1 = calculate_precision_recall_f1([true_ents], [pred_ents],matching=type.upper())\n",
    "        my_new[type]['COR'].append(COR)\n",
    "        my_new[type]['PAR'].append(PAR)\n",
    "        my_new[type]['FP'].append(FP)\n",
    "        my_new[type]['FN'].append(FN)\n",
    "        my_new[type]['precision'].append(prec)\n",
    "        my_new[type]['recall'].append(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc7eb0-fcef-4312-9340-920314f347bb",
   "metadata": {},
   "source": [
    "**Compare**\n",
    "\n",
    "TP = correct + partial\\\n",
    "FP = spurious + incorrect?\\\n",
    "FN = missed + incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e4776bd4-bf80-4dd1-b75b-ae5f1bb50b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(davids['exact']['correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0d39aea2-fc22-4f7b-9826-c317608089f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(my_old['strong']['TP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b24b6541-3e79-4f67-8cc6-7a255a58afea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(my_new['strong']['COR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0aaa0afe-5df5-46b0-b5bb-1820c330bb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my['strong']['FP'][53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a370f4b3-8f05-4506-8216-a7d478a514d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davids['exact']['spurious'][53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d6eb28d5-2c31-44dd-a4ed-471b1bb59395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20616666666666666"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(my['strong']['recall']) # macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fb6a9b0b-aec5-4063-8671-2971d7449e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = gold_df['id'].unique()\n",
    "old_strong = my_old['strong']['TP']\n",
    "new_strong = my_new['strong']['COR']\n",
    "davids_strong = davids['exact']['correct']\n",
    "disc = map(lambda x: \"\" if x==True else \"DISC\", [old_strong[i] == new_strong[i] == davids_strong[i] for i in range(len(doc_ids))])\n",
    "\n",
    "pd.DataFrame({'id':doc_ids,'gold':[gold_df[gold_df['id']==doc_id]['entities'].to_list() for doc_id in doc_ids],'pred':[result_df[result_df[id_col]==doc_id]['entities'].to_list() for doc_id in doc_ids],'My Old':old_strong,'My New':new_strong,'Davids':davids_strong, 'DISC':disc}).to_csv('check_correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6cdd9e8f-c3c8-43cd-a54c-afbc03d43b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = gold_df['id'].unique()\n",
    "old_weak = my_old['weak']['TP']\n",
    "new_weak = [my_new['weak']['COR'][i] + my_new['weak']['PAR'][i] for i in range(len(my_new['weak']['COR']))]\n",
    "davids_weak = [davids['partial']['correct'][i] + davids['partial']['partial'][i] for i in range(len(davids['partial']['correct']))]\n",
    "disc = map(lambda x: \"\" if x==True else \"DISC\", [old_weak[i] == new_weak[i] == davids_weak[i] for i in range(len(doc_ids))])\n",
    "\n",
    "pd.DataFrame({'id':doc_ids,'gold':[gold_df[gold_df['id']==doc_id]['entities'].to_list() for doc_id in doc_ids],'pred':[result_df[result_df[id_col]==doc_id]['entities'].to_list() for doc_id in doc_ids],'My Old':old_weak,'My New':new_weak,'Davids':davids_weak, 'DISC':disc}).to_csv('check_correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "046b6449-69f0-4deb-acff-cdf87bb3a0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct': 2,\n",
       " 'incorrect': 0,\n",
       " 'partial': 0,\n",
       " 'missed': 0,\n",
       " 'spurious': 2,\n",
       " 'precision': 0,\n",
       " 'recall': 0,\n",
       " 'actual': 4,\n",
       " 'possible': 2}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 53\n",
    "tmp_results, tmp_agg_results = compute_metrics(\n",
    "        all_true_ents[i], all_pred_ents[i],  tags\n",
    "    )\n",
    "tmp_results['exact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6f7a6dbe-d88a-4aef-8628-ffad45429f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Entity(e_type='LOC', start_offset=1, end_offset=2),\n",
       " Entity(e_type='PER', start_offset=6, end_offset=7)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_true_ents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8142c027-6e86-4a53-b658-c1d562372ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Entity(e_type='ORG', start_offset=0, end_offset=1),\n",
       " Entity(e_type='ORG', start_offset=1, end_offset=2),\n",
       " Entity(e_type='ORG', start_offset=6, end_offset=7),\n",
       " Entity(e_type='ORG', start_offset=10, end_offset=11)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred_ents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c27011f8-1c32-4963-bc35-b9bb1cd4f6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sample</th>\n",
       "      <th>entities</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>19861114075329I</td>\n",
       "      <td>ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...</td>\n",
       "      <td>TCA</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>19861114075329I</td>\n",
       "      <td>ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...</td>\n",
       "      <td>PILOT</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                             sample  \\\n",
       "78  19861114075329I  ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...   \n",
       "79  19861114075329I  ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...   \n",
       "\n",
       "   entities labels  \n",
       "78      TCA    LOC  \n",
       "79    PILOT    PER  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df[gold_df['id']==gold_df['id'].unique()[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "eef7cf04-a750-4bc8-a584-9be896658a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>c5_unique_id</th>\n",
       "      <th>c119_text</th>\n",
       "      <th>entities</th>\n",
       "      <th>POS tags</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1117</td>\n",
       "      <td>19861114075329I</td>\n",
       "      <td>ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...</td>\n",
       "      <td>ENTERED</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1117</td>\n",
       "      <td>19861114075329I</td>\n",
       "      <td>ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...</td>\n",
       "      <td>TCA</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1117</td>\n",
       "      <td>19861114075329I</td>\n",
       "      <td>ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...</td>\n",
       "      <td>PILOT</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1117</td>\n",
       "      <td>19861114075329I</td>\n",
       "      <td>ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...</td>\n",
       "      <td>MALFUNCTIONING</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index     c5_unique_id  \\\n",
       "236   1117  19861114075329I   \n",
       "237   1117  19861114075329I   \n",
       "238   1117  19861114075329I   \n",
       "239   1117  19861114075329I   \n",
       "\n",
       "                                             c119_text        entities  \\\n",
       "236  ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...         ENTERED   \n",
       "237  ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...             TCA   \n",
       "238  ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...           PILOT   \n",
       "239  ENTERED TCA WITHOUT ATC COMMUNICATION. PILOT W...  MALFUNCTIONING   \n",
       "\n",
       "    POS tags labels  \n",
       "236      NNP    ORG  \n",
       "237      NNP    ORG  \n",
       "238      NNP    ORG  \n",
       "239      NNP    ORG  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[result_df[id_col]==gold_df['id'].unique()[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "91564576-a861-4854-a0f8-ac41862a94d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ENTERED',\n",
       " 1: 'TCA',\n",
       " 2: 'WITHOUT',\n",
       " 3: 'ATC',\n",
       " 4: 'COMMUNICATION',\n",
       " 5: '/.',\n",
       " 6: 'PILOT',\n",
       " 7: 'WAS',\n",
       " 8: 'AWARE',\n",
       " 9: 'OF',\n",
       " 10: 'MALFUNCTIONING',\n",
       " 11: 'ENCODING',\n",
       " 12: 'ALTIMETER',\n",
       " 13: '/.'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faa[gold_df['id'].unique()[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfc388-8464-4e2b-b3db-dbff77e126eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
